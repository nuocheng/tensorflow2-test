{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fceabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "x_data=load_iris().data\n",
    "y_data=load_iris().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e85b2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "#### 将数据打乱\n",
    "np.random.seed(118)\n",
    "np.random.shuffle(x_data)\n",
    "np.random.shuffle(y_data)\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "#将数据分为训练与测试\n",
    "x_train=x_data[:-30]\n",
    "y_train=y_data[:-30]\n",
    "x_test=x_data[-30:]\n",
    "y_test=y_data[-30:]\n",
    "#将数据打包为数据集\n",
    "train_db=tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(32)\n",
    "test_db=tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59205a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:0.22803884744644165\n",
      "epoch:1 loss:0.22411717474460602\n",
      "epoch:2 loss:0.22422218322753906\n",
      "epoch:3 loss:0.22416305541992188\n",
      "epoch:4 loss:0.22408315539360046\n",
      "epoch:5 loss:0.2240055948495865\n",
      "epoch:6 loss:0.22393213212490082\n",
      "epoch:7 loss:0.2238626480102539\n",
      "epoch:8 loss:0.2237968146800995\n",
      "epoch:9 loss:0.22373437881469727\n",
      "epoch:10 loss:0.22367508709430695\n",
      "epoch:11 loss:0.22361867129802704\n",
      "epoch:12 loss:0.2235649973154068\n",
      "epoch:13 loss:0.22351376712322235\n",
      "epoch:14 loss:0.22346486151218414\n",
      "epoch:15 loss:0.22341807186603546\n",
      "epoch:16 loss:0.22337333858013153\n",
      "epoch:17 loss:0.22333039343357086\n",
      "epoch:18 loss:0.22328922152519226\n",
      "epoch:19 loss:0.223249614238739\n",
      "epoch:20 loss:0.22321149706840515\n",
      "epoch:21 loss:0.2231747806072235\n",
      "epoch:22 loss:0.22313936054706573\n",
      "epoch:23 loss:0.22310516238212585\n",
      "epoch:24 loss:0.2230720818042755\n",
      "epoch:25 loss:0.22304004430770874\n",
      "epoch:26 loss:0.22300903499126434\n",
      "epoch:27 loss:0.2229788899421692\n",
      "epoch:28 loss:0.22294963896274567\n",
      "epoch:29 loss:0.2229211926460266\n",
      "epoch:30 loss:0.22289347648620605\n",
      "epoch:31 loss:0.2228664755821228\n",
      "epoch:32 loss:0.22284017503261566\n",
      "epoch:33 loss:0.22281445562839508\n",
      "epoch:34 loss:0.22278934717178345\n",
      "epoch:35 loss:0.2227647602558136\n",
      "epoch:36 loss:0.22274070978164673\n",
      "epoch:37 loss:0.22271712124347687\n",
      "epoch:38 loss:0.22269397974014282\n",
      "epoch:39 loss:0.2226713001728058\n",
      "epoch:40 loss:0.2226489931344986\n",
      "epoch:41 loss:0.22262705862522125\n",
      "epoch:42 loss:0.22260549664497375\n",
      "epoch:43 loss:0.22258424758911133\n",
      "epoch:44 loss:0.22256332635879517\n",
      "epoch:45 loss:0.22254270315170288\n",
      "epoch:46 loss:0.22252236306667328\n",
      "epoch:47 loss:0.22250230610370636\n",
      "epoch:48 loss:0.22248247265815735\n",
      "epoch:49 loss:0.22246289253234863\n",
      "epoch:50 loss:0.22244352102279663\n",
      "epoch:51 loss:0.22242437303066254\n",
      "epoch:52 loss:0.22240540385246277\n",
      "epoch:53 loss:0.22238662838935852\n",
      "epoch:54 loss:0.222368061542511\n",
      "epoch:55 loss:0.2223496437072754\n",
      "epoch:56 loss:0.22233138978481293\n",
      "epoch:57 loss:0.2223132699728012\n",
      "epoch:58 loss:0.222295343875885\n",
      "epoch:59 loss:0.22227750718593597\n",
      "epoch:60 loss:0.22225983440876007\n",
      "epoch:61 loss:0.22224225103855133\n",
      "epoch:62 loss:0.22222481667995453\n",
      "epoch:63 loss:0.22220748662948608\n",
      "epoch:64 loss:0.2221902757883072\n",
      "epoch:65 loss:0.22217315435409546\n",
      "epoch:66 loss:0.2221561223268509\n",
      "epoch:67 loss:0.22213920950889587\n",
      "epoch:68 loss:0.22212237119674683\n",
      "epoch:69 loss:0.22210560739040375\n",
      "epoch:70 loss:0.2220889776945114\n",
      "epoch:71 loss:0.22207236289978027\n",
      "epoch:72 loss:0.2220558375120163\n",
      "epoch:73 loss:0.22203940153121948\n",
      "epoch:74 loss:0.22202304005622864\n",
      "epoch:75 loss:0.22200673818588257\n",
      "epoch:76 loss:0.22199051082134247\n",
      "epoch:77 loss:0.22197431325912476\n",
      "epoch:78 loss:0.2219582200050354\n",
      "epoch:79 loss:0.22194215655326843\n",
      "epoch:80 loss:0.22192615270614624\n",
      "epoch:81 loss:0.22191017866134644\n",
      "epoch:82 loss:0.22189432382583618\n",
      "epoch:83 loss:0.22187846899032593\n",
      "epoch:84 loss:0.22186265885829926\n",
      "epoch:85 loss:0.22184690833091736\n",
      "epoch:86 loss:0.22183121740818024\n",
      "epoch:87 loss:0.22181552648544312\n",
      "epoch:88 loss:0.22179992496967316\n",
      "epoch:89 loss:0.22178435325622559\n",
      "epoch:90 loss:0.2217688262462616\n",
      "epoch:91 loss:0.22175332903862\n",
      "epoch:92 loss:0.22173786163330078\n",
      "epoch:93 loss:0.22172246873378754\n",
      "epoch:94 loss:0.2217070758342743\n",
      "epoch:95 loss:0.22169174253940582\n",
      "epoch:96 loss:0.22167642414569855\n",
      "epoch:97 loss:0.22166115045547485\n",
      "epoch:98 loss:0.22164590656757355\n",
      "epoch:99 loss:0.22163069248199463\n",
      "epoch:100 loss:0.2216155230998993\n",
      "epoch:101 loss:0.22160039842128754\n",
      "epoch:102 loss:0.22158527374267578\n",
      "epoch:103 loss:0.2215701937675476\n",
      "epoch:104 loss:0.22155515849590302\n",
      "epoch:105 loss:0.2215401530265808\n",
      "epoch:106 loss:0.2215251326560974\n",
      "epoch:107 loss:0.2215101569890976\n",
      "epoch:108 loss:0.22149525582790375\n",
      "epoch:109 loss:0.2214803397655487\n",
      "epoch:110 loss:0.22146545350551605\n",
      "epoch:111 loss:0.22145062685012817\n",
      "epoch:112 loss:0.2214358150959015\n",
      "epoch:113 loss:0.2214210033416748\n",
      "epoch:114 loss:0.2214062213897705\n",
      "epoch:115 loss:0.221391499042511\n",
      "epoch:116 loss:0.22137679159641266\n",
      "epoch:117 loss:0.22136205434799194\n",
      "epoch:118 loss:0.2213473916053772\n",
      "epoch:119 loss:0.22133278846740723\n",
      "epoch:120 loss:0.22131814062595367\n",
      "epoch:121 loss:0.2213035672903061\n",
      "epoch:122 loss:0.2212889939546585\n",
      "epoch:123 loss:0.2212744504213333\n",
      "epoch:124 loss:0.2212599366903305\n",
      "epoch:125 loss:0.22124545276165009\n",
      "epoch:126 loss:0.22123098373413086\n",
      "epoch:127 loss:0.22121651470661163\n",
      "epoch:128 loss:0.22120210528373718\n",
      "epoch:129 loss:0.22118769586086273\n",
      "epoch:130 loss:0.22117331624031067\n",
      "epoch:131 loss:0.221158966422081\n",
      "epoch:132 loss:0.22114461660385132\n",
      "epoch:133 loss:0.22113031148910522\n",
      "epoch:134 loss:0.22111603617668152\n",
      "epoch:135 loss:0.2211017608642578\n",
      "epoch:136 loss:0.2210875153541565\n",
      "epoch:137 loss:0.22107328474521637\n",
      "epoch:138 loss:0.22105908393859863\n",
      "epoch:139 loss:0.22104491293430328\n",
      "epoch:140 loss:0.22103077173233032\n",
      "epoch:141 loss:0.22101661562919617\n",
      "epoch:142 loss:0.2210025191307068\n",
      "epoch:143 loss:0.2209884375333786\n",
      "epoch:144 loss:0.22097435593605042\n",
      "epoch:145 loss:0.22096030414104462\n",
      "epoch:146 loss:0.22094625234603882\n",
      "epoch:147 loss:0.2209322601556778\n",
      "epoch:148 loss:0.22091828286647797\n",
      "epoch:149 loss:0.22090432047843933\n",
      "epoch:150 loss:0.2208903729915619\n",
      "epoch:151 loss:0.22087642550468445\n",
      "epoch:152 loss:0.22086255252361298\n",
      "epoch:153 loss:0.2208486646413803\n",
      "epoch:154 loss:0.22083479166030884\n",
      "epoch:155 loss:0.22082096338272095\n",
      "epoch:156 loss:0.22080713510513306\n",
      "epoch:157 loss:0.22079333662986755\n",
      "epoch:158 loss:0.22077955305576324\n",
      "epoch:159 loss:0.22076578438282013\n",
      "epoch:160 loss:0.2207520604133606\n",
      "epoch:161 loss:0.22073833644390106\n",
      "epoch:162 loss:0.22072464227676392\n",
      "epoch:163 loss:0.22071094810962677\n",
      "epoch:164 loss:0.2206972986459732\n",
      "epoch:165 loss:0.22068364918231964\n",
      "epoch:166 loss:0.22067004442214966\n",
      "epoch:167 loss:0.22065642476081848\n",
      "epoch:168 loss:0.2206428498029709\n",
      "epoch:169 loss:0.22062928974628448\n",
      "epoch:170 loss:0.22061575949192047\n",
      "epoch:171 loss:0.22060221433639526\n",
      "epoch:172 loss:0.22058874368667603\n",
      "epoch:173 loss:0.2205752283334732\n",
      "epoch:174 loss:0.22056177258491516\n",
      "epoch:175 loss:0.22054831683635712\n",
      "epoch:176 loss:0.22053489089012146\n",
      "epoch:177 loss:0.22052150964736938\n",
      "epoch:178 loss:0.22050809860229492\n",
      "epoch:179 loss:0.22049474716186523\n",
      "epoch:180 loss:0.22048139572143555\n",
      "epoch:181 loss:0.22046805918216705\n",
      "epoch:182 loss:0.22045472264289856\n",
      "epoch:183 loss:0.22044144570827484\n",
      "epoch:184 loss:0.22042818367481232\n",
      "epoch:185 loss:0.2204149216413498\n",
      "epoch:186 loss:0.22040168941020966\n",
      "epoch:187 loss:0.2203884720802307\n",
      "epoch:188 loss:0.22037526965141296\n",
      "epoch:189 loss:0.2203621119260788\n",
      "epoch:190 loss:0.22034893929958344\n",
      "epoch:191 loss:0.22033578157424927\n",
      "epoch:192 loss:0.22032266855239868\n",
      "epoch:193 loss:0.2203095704317093\n",
      "epoch:194 loss:0.2202964723110199\n",
      "epoch:195 loss:0.2202834039926529\n",
      "epoch:196 loss:0.22027036547660828\n",
      "epoch:197 loss:0.22025734186172485\n",
      "epoch:198 loss:0.22024431824684143\n",
      "epoch:199 loss:0.2202313244342804\n",
      "epoch:200 loss:0.22021834552288055\n",
      "epoch:201 loss:0.2202053815126419\n",
      "epoch:202 loss:0.22019243240356445\n",
      "epoch:203 loss:0.2201795130968094\n",
      "epoch:204 loss:0.2201666235923767\n",
      "epoch:205 loss:0.22015371918678284\n",
      "epoch:206 loss:0.22014085948467255\n",
      "epoch:207 loss:0.22012801468372345\n",
      "epoch:208 loss:0.22011518478393555\n",
      "epoch:209 loss:0.22010235488414764\n",
      "epoch:210 loss:0.22008958458900452\n",
      "epoch:211 loss:0.2200767695903778\n",
      "epoch:212 loss:0.22006402909755707\n",
      "epoch:213 loss:0.22005128860473633\n",
      "epoch:214 loss:0.22003857791423798\n",
      "epoch:215 loss:0.22002583742141724\n",
      "epoch:216 loss:0.22001314163208008\n",
      "epoch:217 loss:0.2200004756450653\n",
      "epoch:218 loss:0.21998782455921173\n",
      "epoch:219 loss:0.21997518837451935\n",
      "epoch:220 loss:0.21996256709098816\n",
      "epoch:221 loss:0.21994996070861816\n",
      "epoch:222 loss:0.21993738412857056\n",
      "epoch:223 loss:0.21992480754852295\n",
      "epoch:224 loss:0.21991226077079773\n",
      "epoch:225 loss:0.2198997139930725\n",
      "epoch:226 loss:0.21988719701766968\n",
      "epoch:227 loss:0.21987470984458923\n",
      "epoch:228 loss:0.2198622226715088\n",
      "epoch:229 loss:0.21984976530075073\n",
      "epoch:230 loss:0.21983730792999268\n",
      "epoch:231 loss:0.2198248654603958\n",
      "epoch:232 loss:0.21981246769428253\n",
      "epoch:233 loss:0.21980006992816925\n",
      "epoch:234 loss:0.21978767216205597\n",
      "epoch:235 loss:0.21977530419826508\n",
      "epoch:236 loss:0.21976298093795776\n",
      "epoch:237 loss:0.21975064277648926\n",
      "epoch:238 loss:0.21973831951618195\n",
      "epoch:239 loss:0.21972602605819702\n",
      "epoch:240 loss:0.21971376240253448\n",
      "epoch:241 loss:0.21970148384571075\n",
      "epoch:242 loss:0.2196892648935318\n",
      "epoch:243 loss:0.21967700123786926\n",
      "epoch:244 loss:0.2196648120880127\n",
      "epoch:245 loss:0.21965262293815613\n",
      "epoch:246 loss:0.21964043378829956\n",
      "epoch:247 loss:0.2196282595396042\n",
      "epoch:248 loss:0.2196161150932312\n",
      "epoch:249 loss:0.2196039855480194\n",
      "epoch:250 loss:0.21959188580513\n",
      "epoch:251 loss:0.2195797711610794\n",
      "epoch:252 loss:0.21956771612167358\n",
      "epoch:253 loss:0.21955564618110657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:254 loss:0.21954359114170074\n",
      "epoch:255 loss:0.21953153610229492\n",
      "epoch:256 loss:0.21951954066753387\n",
      "epoch:257 loss:0.21950751543045044\n",
      "epoch:258 loss:0.21949556469917297\n",
      "epoch:259 loss:0.21948358416557312\n",
      "epoch:260 loss:0.21947161853313446\n",
      "epoch:261 loss:0.21945969760417938\n",
      "epoch:262 loss:0.2194477766752243\n",
      "epoch:263 loss:0.21943587064743042\n",
      "epoch:264 loss:0.21942397952079773\n",
      "epoch:265 loss:0.21941210329532623\n",
      "epoch:266 loss:0.21940022706985474\n",
      "epoch:267 loss:0.21938841044902802\n",
      "epoch:268 loss:0.2193765938282013\n",
      "epoch:269 loss:0.21936477720737457\n",
      "epoch:270 loss:0.21935297548770905\n",
      "epoch:271 loss:0.2193412035703659\n",
      "epoch:272 loss:0.21932943165302277\n",
      "epoch:273 loss:0.21931768953800201\n",
      "epoch:274 loss:0.21930594742298126\n",
      "epoch:275 loss:0.2192942500114441\n",
      "epoch:276 loss:0.21928252279758453\n",
      "epoch:277 loss:0.21927087008953094\n",
      "epoch:278 loss:0.21925917267799377\n",
      "epoch:279 loss:0.21924751996994019\n",
      "epoch:280 loss:0.21923589706420898\n",
      "epoch:281 loss:0.21922427415847778\n",
      "epoch:282 loss:0.21921265125274658\n",
      "epoch:283 loss:0.21920105814933777\n",
      "epoch:284 loss:0.21918949484825134\n",
      "epoch:285 loss:0.21917793154716492\n",
      "epoch:286 loss:0.21916638314723969\n",
      "epoch:287 loss:0.21915484964847565\n",
      "epoch:288 loss:0.2191433310508728\n",
      "epoch:289 loss:0.21913181245326996\n",
      "epoch:290 loss:0.2191203385591507\n",
      "epoch:291 loss:0.21910887956619263\n",
      "epoch:292 loss:0.21909740567207336\n",
      "epoch:293 loss:0.21908597648143768\n",
      "epoch:294 loss:0.219074547290802\n",
      "epoch:295 loss:0.2190631479024887\n",
      "epoch:296 loss:0.21905174851417542\n",
      "epoch:297 loss:0.21904036402702332\n",
      "epoch:298 loss:0.2190289944410324\n",
      "epoch:299 loss:0.2190176397562027\n",
      "epoch:300 loss:0.21900632977485657\n",
      "epoch:301 loss:0.21899498999118805\n",
      "epoch:302 loss:0.21898368000984192\n",
      "epoch:303 loss:0.21897238492965698\n",
      "epoch:304 loss:0.21896108984947205\n",
      "epoch:305 loss:0.2189498245716095\n",
      "epoch:306 loss:0.21893857419490814\n",
      "epoch:307 loss:0.21892733871936798\n",
      "epoch:308 loss:0.2189161330461502\n",
      "epoch:309 loss:0.21890491247177124\n",
      "epoch:310 loss:0.21889373660087585\n",
      "epoch:311 loss:0.21888256072998047\n",
      "epoch:312 loss:0.21887139976024628\n",
      "epoch:313 loss:0.21886023879051208\n",
      "epoch:314 loss:0.21884912252426147\n",
      "epoch:315 loss:0.21883799135684967\n",
      "epoch:316 loss:0.21882688999176025\n",
      "epoch:317 loss:0.21881578862667084\n",
      "epoch:318 loss:0.21880470216274261\n",
      "epoch:319 loss:0.21879366040229797\n",
      "epoch:320 loss:0.21878260374069214\n",
      "epoch:321 loss:0.2187715768814087\n",
      "epoch:322 loss:0.21876055002212524\n",
      "epoch:323 loss:0.2187495231628418\n",
      "epoch:324 loss:0.21873855590820312\n",
      "epoch:325 loss:0.21872757375240326\n",
      "epoch:326 loss:0.2187165915966034\n",
      "epoch:327 loss:0.2187056541442871\n",
      "epoch:328 loss:0.21869473159313202\n",
      "epoch:329 loss:0.21868379414081573\n",
      "epoch:330 loss:0.21867291629314423\n",
      "epoch:331 loss:0.21866196393966675\n",
      "epoch:332 loss:0.21865111589431763\n",
      "epoch:333 loss:0.2186402529478073\n",
      "epoch:334 loss:0.218629390001297\n",
      "epoch:335 loss:0.21861855685710907\n",
      "epoch:336 loss:0.21860773861408234\n",
      "epoch:337 loss:0.2185969054698944\n",
      "epoch:338 loss:0.21858611702919006\n",
      "epoch:339 loss:0.2185753434896469\n",
      "epoch:340 loss:0.21856456995010376\n",
      "epoch:341 loss:0.2185538113117218\n",
      "epoch:342 loss:0.21854306757450104\n",
      "epoch:343 loss:0.21853235363960266\n",
      "epoch:344 loss:0.2185216099023819\n",
      "epoch:345 loss:0.2185109257698059\n",
      "epoch:346 loss:0.21850024163722992\n",
      "epoch:347 loss:0.21848955750465393\n",
      "epoch:348 loss:0.21847888827323914\n",
      "epoch:349 loss:0.21846823394298553\n",
      "epoch:350 loss:0.21845759451389313\n",
      "epoch:351 loss:0.21844696998596191\n",
      "epoch:352 loss:0.2184363752603531\n",
      "epoch:353 loss:0.21842576563358307\n",
      "epoch:354 loss:0.21841520071029663\n",
      "epoch:355 loss:0.2184046357870102\n",
      "epoch:356 loss:0.21839407086372375\n",
      "epoch:357 loss:0.2183835208415985\n",
      "epoch:358 loss:0.21837300062179565\n",
      "epoch:359 loss:0.2183624655008316\n",
      "epoch:360 loss:0.21835196018218994\n",
      "epoch:361 loss:0.21834146976470947\n",
      "epoch:362 loss:0.2183309942483902\n",
      "epoch:363 loss:0.2183205485343933\n",
      "epoch:364 loss:0.21831008791923523\n",
      "epoch:365 loss:0.21829965710639954\n",
      "epoch:366 loss:0.21828921139240265\n",
      "epoch:367 loss:0.21827881038188934\n",
      "epoch:368 loss:0.21826839447021484\n",
      "epoch:369 loss:0.21825802326202393\n",
      "epoch:370 loss:0.218247652053833\n",
      "epoch:371 loss:0.21823729574680328\n",
      "epoch:372 loss:0.21822693943977356\n",
      "epoch:373 loss:0.21821659803390503\n",
      "epoch:374 loss:0.2182062566280365\n",
      "epoch:375 loss:0.21819595992565155\n",
      "epoch:376 loss:0.2181856781244278\n",
      "epoch:377 loss:0.21817535161972046\n",
      "epoch:378 loss:0.2181650847196579\n",
      "epoch:379 loss:0.21815484762191772\n",
      "epoch:380 loss:0.21814458072185516\n",
      "epoch:381 loss:0.218134343624115\n",
      "epoch:382 loss:0.218124121427536\n",
      "epoch:383 loss:0.21811392903327942\n",
      "epoch:384 loss:0.21810372173786163\n",
      "epoch:385 loss:0.21809354424476624\n",
      "epoch:386 loss:0.21808336675167084\n",
      "epoch:387 loss:0.21807318925857544\n",
      "epoch:388 loss:0.21806304156780243\n",
      "epoch:389 loss:0.2180529236793518\n",
      "epoch:390 loss:0.21804279088974\n",
      "epoch:391 loss:0.21803268790245056\n",
      "epoch:392 loss:0.21802258491516113\n",
      "epoch:393 loss:0.2180124968290329\n",
      "epoch:394 loss:0.21800243854522705\n",
      "epoch:395 loss:0.21799236536026\n",
      "epoch:396 loss:0.21798232197761536\n",
      "epoch:397 loss:0.2179722785949707\n",
      "epoch:398 loss:0.21796223521232605\n",
      "epoch:399 loss:0.21795223653316498\n",
      "epoch:400 loss:0.2179422229528427\n",
      "epoch:401 loss:0.21793223917484283\n",
      "epoch:402 loss:0.21792227029800415\n",
      "epoch:403 loss:0.21791228652000427\n",
      "epoch:404 loss:0.21790234744548798\n",
      "epoch:405 loss:0.21789239346981049\n",
      "epoch:406 loss:0.2178824543952942\n",
      "epoch:407 loss:0.21787256002426147\n",
      "epoch:408 loss:0.21786266565322876\n",
      "epoch:409 loss:0.21785275638103485\n",
      "epoch:410 loss:0.21784287691116333\n",
      "epoch:411 loss:0.217833012342453\n",
      "epoch:412 loss:0.21782316267490387\n",
      "epoch:413 loss:0.21781331300735474\n",
      "epoch:414 loss:0.2178034633398056\n",
      "epoch:415 loss:0.21779364347457886\n",
      "epoch:416 loss:0.2177838236093521\n",
      "epoch:417 loss:0.21777403354644775\n",
      "epoch:418 loss:0.2177642285823822\n",
      "epoch:419 loss:0.21775445342063904\n",
      "epoch:420 loss:0.21774469316005707\n",
      "epoch:421 loss:0.2177349478006363\n",
      "epoch:422 loss:0.21772520244121552\n",
      "epoch:423 loss:0.21771547198295593\n",
      "epoch:424 loss:0.21770575642585754\n",
      "epoch:425 loss:0.21769602596759796\n",
      "epoch:426 loss:0.21768635511398315\n",
      "epoch:427 loss:0.21767666935920715\n",
      "epoch:428 loss:0.21766699850559235\n",
      "epoch:429 loss:0.21765732765197754\n",
      "epoch:430 loss:0.21764765679836273\n",
      "epoch:431 loss:0.2176380455493927\n",
      "epoch:432 loss:0.21762841939926147\n",
      "epoch:433 loss:0.21761877834796906\n",
      "epoch:434 loss:0.2176091969013214\n",
      "epoch:435 loss:0.21759960055351257\n",
      "epoch:436 loss:0.21759001910686493\n",
      "epoch:437 loss:0.21758045256137848\n",
      "epoch:438 loss:0.21757087111473083\n",
      "epoch:439 loss:0.21756134927272797\n",
      "epoch:440 loss:0.2175517976284027\n",
      "epoch:441 loss:0.21754226088523865\n",
      "epoch:442 loss:0.21753275394439697\n",
      "epoch:443 loss:0.2175232619047165\n",
      "epoch:444 loss:0.21751375496387482\n",
      "epoch:445 loss:0.21750429272651672\n",
      "epoch:446 loss:0.21749480068683624\n",
      "epoch:447 loss:0.21748535335063934\n",
      "epoch:448 loss:0.21747592091560364\n",
      "epoch:449 loss:0.21746645867824554\n",
      "epoch:450 loss:0.21745702624320984\n",
      "epoch:451 loss:0.21744762361049652\n",
      "epoch:452 loss:0.217438206076622\n",
      "epoch:453 loss:0.21742883324623108\n",
      "epoch:454 loss:0.21741944551467896\n",
      "epoch:455 loss:0.21741005778312683\n",
      "epoch:456 loss:0.2174006998538971\n",
      "epoch:457 loss:0.21739134192466736\n",
      "epoch:458 loss:0.21738201379776\n",
      "epoch:459 loss:0.21737270057201385\n",
      "epoch:460 loss:0.2173633575439453\n",
      "epoch:461 loss:0.21735405921936035\n",
      "epoch:462 loss:0.2173447608947754\n",
      "epoch:463 loss:0.21733547747135162\n",
      "epoch:464 loss:0.21732620894908905\n",
      "epoch:465 loss:0.21731694042682648\n",
      "epoch:466 loss:0.2173076868057251\n",
      "epoch:467 loss:0.2172984480857849\n",
      "epoch:468 loss:0.21728920936584473\n",
      "epoch:469 loss:0.21727998554706573\n",
      "epoch:470 loss:0.21727079153060913\n",
      "epoch:471 loss:0.21726156771183014\n",
      "epoch:472 loss:0.21725238859653473\n",
      "epoch:473 loss:0.21724320948123932\n",
      "epoch:474 loss:0.2172340452671051\n",
      "epoch:475 loss:0.21722489595413208\n",
      "epoch:476 loss:0.21721571683883667\n",
      "epoch:477 loss:0.21720658242702484\n",
      "epoch:478 loss:0.2171974629163742\n",
      "epoch:479 loss:0.21718832850456238\n",
      "epoch:480 loss:0.21717923879623413\n",
      "epoch:481 loss:0.21717016398906708\n",
      "epoch:482 loss:0.21716104447841644\n",
      "epoch:483 loss:0.21715198457241058\n",
      "epoch:484 loss:0.21714289486408234\n",
      "epoch:485 loss:0.21713386476039886\n",
      "epoch:486 loss:0.217124804854393\n",
      "epoch:487 loss:0.21711577475070953\n",
      "epoch:488 loss:0.21710675954818726\n",
      "epoch:489 loss:0.2170977145433426\n",
      "epoch:490 loss:0.2170887291431427\n",
      "epoch:491 loss:0.2170797437429428\n",
      "epoch:492 loss:0.21707074344158173\n",
      "epoch:493 loss:0.21706178784370422\n",
      "epoch:494 loss:0.21705280244350433\n",
      "epoch:495 loss:0.21704384684562683\n",
      "epoch:496 loss:0.21703489124774933\n",
      "epoch:497 loss:0.2170259803533554\n",
      "epoch:498 loss:0.2170170545578003\n",
      "epoch:499 loss:0.21700814366340637\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "lr=0.1\n",
    "loss_all=[]\n",
    "acc=[]\n",
    "epochs=500\n",
    "w=tf.Variable(tf.random.normal([4,3],mean=0.1,stddev=0.1))\n",
    "b=tf.Variable(tf.random.normal([3],stddev=0.1))\n",
    "for epoch in range(epochs):\n",
    "    for step,(x_train,y_train) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            x_train=tf.cast(x_train,dtype=w.dtype)\n",
    "            y_train=tf.cast(y_train,dtype=tf.int32)\n",
    "            y=tf.matmul(x_train,w)+b\n",
    "            y=tf.nn.softmax(y)\n",
    "            y_train_one_hot=tf.one_hot(y_train,depth=3)\n",
    "            loss=tf.reduce_mean(tf.pow((y-y_train_one_hot),2))\n",
    "            loss_all.append(loss.numpy())\n",
    "        grad=tape.gradient(loss,[w,b])\n",
    "        w.assign_sub(lr*grad[0])\n",
    "        b.assign_sub(lr*grad[1])\n",
    "    print(\"epoch:{}  train_loss:{}\".format(epoch,loss.numpy())) \n",
    "    \n",
    "    for (x_test,y_test) in test_db:\n",
    "        x_test=tf.cast(x_test,dtype=w.dtype)\n",
    "        y_test=tf.cast(y_test,dtype=tf.int32)\n",
    "        pre=tf.matmul(x_test,w)+b\n",
    "        tf.reduce_sum(tf.equal(tf.argmax(pre,axis=1),y_test),dtype=tf.int32)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e072d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
